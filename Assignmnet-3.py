# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lzTzuu4g1DmTblcItTAMojYSm-7ZJ7kD
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
from gensim.models import Word2Vec
from nltk.corpus import brown
from nltk.corpus import inaugural
import nltk
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer 
nltk.download('brown')
nltk.download('inaugural')
import plotly.express as px
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
from sklearn.feature_extraction.text import TfidfVectorizer

sents = brown.sents()
sents
words = word_tokenize(sents)
sents = sents.lower()
stop_words = set(stopwords.words('english')) 
word_tokens = word_tokenize(sents)
New_Filter_sentence = [w for w in word_tokens if not w in stop_words] 
port_stemmer = PorterStemmer()
for word in sents.split():
  print(ps.stem(word))
Sents_Lematizer = WordNetLemmatizer()  
windows = [1, 2, 5, 10]
vectors = [10, 50, 100, 300]
for i in windows:
  for v in vectors:
    w2v_model = Word2Vec(window=i, size=v)
    w2v_model.build_vocab(sents)
    w2v_model.train(sents, total_examples=w2v_model.corpus_count, epochs=1000, report_delay=1)
    w2v_model.save('FILE')
w2v_model = Word2Vec.load("FILE")  
corpus_news = brown.sents(categories=['news'])
corpus_government = brown.sents(categories=['government'])  
vector = model.wv[sentences=brown.sents(categories=['news','government'])]
sims = model.wv.most_similar(sentences=brown.sents(categories=['news','government']), topn=10)    



sents_a = inaugural.sents()
sents_a
words_a = word_tokenize(sents_a)
sents_a = sents_a.lower()
stop_words_a = set(stopwords.words_a('english')) 
word_tokens_a = word_tokenize(sents_a)
New_Filter_sentence_a = [w for w in word_tokens_a if not w in stop_words_a] 
port_stemmer_a = PorterStemmer()
for word_a in sents_a.split():
  print(ps.stem(word_a))
Sents_Lematizer_a = WordNetLemmatizer()  
windows = [1, 2, 5, 10]
vectors = [10, 50, 100, 300]
for i in windows:
  for v in vectors:
    w2v_model_a = Word2Vec(window=i, size=v)
    w2v_model_a.build_vocab(sents_a)
    w2v_model_a.train(sents_a, total_examples=w2v_model.corpus_count, epochs=1000, report_delay=1)
    w2v_model_a.save('FILE_Inangural')
w2v_model_a = Word2Vec.load("FILE_Inangural")    
vector_a = model.wv[]
sims_a = model.wv.most_similar(()), topn=10)  

vectorizer = TfidfVectorizer()
analyze = vectorizer.build_analyzer()
X = vectorizer.fit_transform()
vec = TfidfVectorizer(use_idf=True, ngram_range=(1,1))
met = vec.fit_transform(text)
cosine_sim = cosine_similarity(met,met)
tfidf_word = vec.get_feature_names()

SimlexData = list()

fo = open("data\\SimLex-999.txt", "r")
for l in fo.readlines():
        SimlexData.append(l.split('\t'))   
        
fo.close()
df = pd.DataFrame(SimlexData[1:], columns=SimlexData[0])
word1_new = []
for i in df.word1.unique():
    word1_new.append(i)
       
word2_new = []
for i in df.word2.unique():
    word2_new.append(i)

for i in word2_new:
    if(i not in word1_new):
        word1_new.append(i)
def similar_word_simlex(word, df):
    simlex_dict = dict()
    for x,y in enumerate(df.word1):
        if(word==y):
            simlex_dict[df.word2[x]] = df.SimLex999[x]
    for x,y in enumerate(df.word2):
        if(word==y):
            simlex_dict[df.word1[x]] = df.SimLex999[x]
    simlex = {k:v for k,v in sorted(simlex_dict.items(), key=lambda x:x[1], reverse=True)}
    return [i for i in simlex.keys()]

top_10_gold = dict()
for i in word1_new:
    top_10_gold[i] = similar_word_simlex(i, df)

sns.set_style('darkgrid')
matplotlib.rcParams['font.size'] = 14
matplotlib.rcParams['figure.figsize'] = (10,6)
matplotlib.rcParams['figure.facecolor'] = '#000'



